# Pipeline Architecture Findings - Systematic Analysis

**Date**: 2025-11-14
**Scope**: System-wide analysis, not article-specific
**Objective**: Identify fundamental architectural issues causing failures across ALL articles

---

## üéØ Executive Summary

After analyzing cross-run consistency, specific claim failures, and the codebase architecture, I've identified:

1. **Good News**: Temperature fix improved consistency (28/100 ‚Üí 68/100)
2. **Bad News**: ~55% uncertainty rate indicates systematic issues
3. **Critical Finding**: **Relevance filtering feature EXISTS but is DISABLED**
4. **Root Cause**: Pipeline architecture has no gatekeeper between search and NLI

---

## üìä Performance Metrics

### Latest Run (Report 9f71178f): 68/100

**Claim Breakdown**:
- ‚úÖ **36.4% SUPPORTED** (4/11 claims) - Claims 1, 2, 10, 11
- ‚ö†Ô∏è **54.5% UNCERTAIN** (6/11 claims) - Claims 3, 4, 5, 6, 7, 8
- ‚ùå **9.1% CONTRADICTED** (1/11 claims) - Claim 9 (FALSE POSITIVE)

**Uncertainty Breakdown**:
- `insufficient_evidence`: 18% (2/11) - Not enough sources found
- `conflicting_expert_opinion`: 18% (2/11) - Sources contradict each other
- `uncertain`: 18% (2/11) - Evidence unclear/ambiguous

**Comparison to Previous Runs**:
- Score range: 28-68/100 (140% variance before temperature fix)
- After temperature=0.0: Scores more stable (68/100)
- But claim-level verdicts still inconsistent across runs

---

## üîç Six Systematic Failure Modes

### 1. OFF-TOPIC Evidence Treated as Contradiction (~30% of failures)

**What happens**: Evidence discusses different aspect of topic ‚Üí NLI scores as CONTRADICTING ‚Üí Judge marks claim as contradicted

**Example**:
- Claim: "Person sent a letter"
- Evidence: "Project doesn't require approval"
- NLI Output: 0.80 contradiction ‚ùå
- Should be: 0.90 neutral ‚úÖ

**Root Cause**: No relevance pre-filter before NLI

**Affected Claims**: Procedural actions, specific events, people-based claims

---

### 2. Temporal Mismatches Not Detected (~10-15% of failures)

**What happens**: Historical sources mixed with current claims ‚Üí False contradictions

**Example**:
- Claim: "Will Scharf heads commission (2025)"
- Evidence: Nixon Library archives (1969-1972)
- System: Treats as relevant ‚Üí FALSE CONTRADICTION

**Root Cause**: Temporal filter exists (`ENABLE_TEMPORAL_CONTEXT=true`) but doesn't catch historical archives

**Affected Claims**: Current office holders, recent events

---

### 3. Legal/Technical Language Ambiguity (~10-20% of failures)

**What happens**: Different phrasings for same concept ‚Üí False contradictions

**Example**:
- Claim: "Exempts from its provisions"
- Evidence A: "Exempts from National Historic Preservation Act"
- Evidence B: "Exempts from Section 106"
- System: Interprets different sections as contradiction

**Root Cause**: No semantic normalization for legal/technical terms

**Affected Claims**: Legal claims, technical specifications

---

### 4. Meta-Content vs Primary Content Confusion (~15-20% of failures)

**What happens**: Search returns news ABOUT topic instead of PRIMARY sources

**Example**:
- Claim: "Huffman sent a letter"
- Search Returns: News articles ABOUT the letter
- Misses: Huffman's official website or the letter itself

**Root Cause**: Search optimization doesn't prioritize primary sources

**Affected Claims**: Actions by sources, documents/letters, official statements

---

### 5. Narrow Search Results for Specific Entities (~10-15% of failures)

**What happens**: Specific names/organizations ‚Üí Too few sources ‚Üí INSUFFICIENT_EVIDENCE

**Example**:
- Claim: "Sara Bronin chaired council"
- Sources Found: 2 (Cornell Law, ACHP official site - both HIGH quality)
- Verdict: INSUFFICIENT_EVIDENCE (need 3)

**Root Cause**: Abstention threshold (MIN_SOURCES_FOR_VERDICT=3) too rigid

**Affected Claims**: Specific individuals, niche organizations

---

### 6. Partial Information Treated as Contradiction (~10-15% of failures)

**What happens**: Evidence addresses only part of multi-part claim ‚Üí Treated as contradiction

**Example**:
- Claim: "Received $350M in donations"
- Evidence: "Project costs $300M"
- System: Cost ‚â† donations ‚Üí Possible contradiction

**Root Cause**: Multi-part claims not decomposed

**Affected Claims**: Claims with multiple assertions (X AND Y)

---

## üö® Critical Discovery: Disabled Features

### Feature #1: Evidence Relevance Filter (EXISTS BUT DISABLED!)

**Config Location**: `backend/app/core/config.py:113-114`

```python
ENABLE_EVIDENCE_RELEVANCE_FILTER: bool = Field(False, env="ENABLE_EVIDENCE_RELEVANCE_FILTER")
RELEVANCE_THRESHOLD: float = Field(0.65, env="RELEVANCE_THRESHOLD")
```

**Status**:
- ‚úÖ Feature flag exists
- ‚úÖ Threshold defined (0.65)
- ‚ùå **NOT in `.env` file** ‚Üí Defaults to `False`
- ‚ùå **NOT actually implemented in pipeline code!**

**Where it SHOULD be used**: `backend/app/pipeline/verify.py` (before NLI)

**Impact if enabled**: Would fix Failure Modes #1 and #4 (~40-50% of failures)

---

### Feature #2: Temporal Context (ENABLED AND IMPLEMENTED)

**Config**: `ENABLE_TEMPORAL_CONTEXT=true` ‚úÖ

**Implementation**: `backend/app/utils/temporal.py` ‚úÖ

**Usage**: `backend/app/pipeline/retrieve.py:362-369` ‚úÖ

**Status**: WORKING but may need tuning for historical archives

---

### Feature #3: Cross-Encoder Reranking (ENABLED)

**Config**: `ENABLE_CROSS_ENCODER_RERANK=true` ‚úÖ

**Implementation**: `backend/app/pipeline/retrieve.py:246-311` ‚úÖ

**Status**: WORKING - reranks evidence by relevance after bi-encoder

**Performance**: Logs show ~50ms latency for 10 evidence pairs ‚úÖ

---

### Feature #4: Domain Credibility Framework (ENABLED)

**Config**: `ENABLE_DOMAIN_CREDIBILITY_FRAMEWORK=true` ‚úÖ

**Implementation**: `backend/app/services/source_credibility.py` (exists)

**Status**: WORKING - filters low-credibility sources

---

### Feature #5: Abstention Logic (ENABLED)

**Config**: `ENABLE_ABSTENTION_LOGIC=true` ‚úÖ

**Thresholds**:
- `MIN_SOURCES_FOR_VERDICT=3` ‚Üê May be too rigid
- `MIN_CREDIBILITY_THRESHOLD=0.75`
- `MIN_CONSENSUS_STRENGTH=0.45`

**Status**: WORKING but causing false negatives (2 high-quality sources rejected)

---

## üèóÔ∏è Pipeline Architecture Analysis

### Current Flow

```
1. Extract Claims (LLM)
     ‚Üì
2. Search for Evidence (Brave/SerpAPI)
     ‚Üì
3. Extract Snippets (Word overlap scoring) ‚Üê ISSUE: Not semantic
     ‚Üì
4. Rank by Embeddings (Bi-encoder)
     ‚Üì
5. Rerank by Cross-Encoder ‚úÖ GOOD
     ‚Üì
6. Apply Credibility Filter ‚úÖ GOOD
     ‚Üì
7. Apply Temporal Filter ‚úÖ GOOD
     ‚Üì
8. Deduplication ‚úÖ GOOD
     ‚Üì
9. NLI Verification (ALL evidence, no relevance filter) ‚Üê ISSUE: No gatekeeper!
     ‚Üì
10. Judge (trusts NLI scores) ‚Üê ISSUE: Assumes evidence is relevant
```

### Problems Identified

**Problem #1: Step 3 - Snippet Extraction**
- Uses word overlap (`claim_words ‚à© sentence_words`)
- Should use semantic similarity
- **Impact**: Extracts wrong sentences from correct sources

**Problem #2: Step 9 - No Relevance Gate**
- NLI runs on ALL evidence (including off-topic)
- No pre-filter for relevance
- **Impact**: Off-topic evidence scored as contradicting

**Problem #3: Step 10 - Judge Trusts NLI Blindly**
- Judge assumes all evidence is relevant
- No mechanism to detect off-topic evidence
- **Impact**: False contradictions from irrelevant evidence

---

### Proposed Improved Flow

```
1. Extract Claims (LLM)
     ‚Üì
2. Search for Evidence (Brave/SerpAPI)
     ‚Üì
3. Extract Snippets (SEMANTIC similarity) ‚Üê FIX: Use embeddings
     ‚Üì
4. Rank by Embeddings (Bi-encoder)
     ‚Üì
5. Rerank by Cross-Encoder
     ‚Üì
6. Apply Credibility Filter
     ‚Üì
7. Apply Temporal Filter
     ‚Üì
8. Deduplication
     ‚Üì
9. ‚ú® NEW: Relevance Filter (semantic similarity > threshold)
     ‚Üì
10. NLI Verification (ONLY ON RELEVANT evidence)
     ‚Üì
11. Judge (with relevance scores available)
```

**Key Changes**:
- Step 3: Use semantic similarity instead of word overlap
- Step 9: Add relevance filter BEFORE NLI
- Step 11: Give judge relevance scores for each evidence piece

---

## üí° Key Insights

### Insight #1: The "80/20" of Failures

**80% of failures come from 2 root causes**:
1. No relevance filtering before NLI (~40%)
2. Word overlap instead of semantic similarity in snippet extraction (~40%)

**Implication**: Fix these 2, and accuracy improves from ~45% ‚Üí ~75%+

---

### Insight #2: Features Exist But Aren't Connected

**Situation**:
- Cross-encoder reranking: ‚úÖ ENABLED, calculates relevance
- Relevance filtering: ‚ùå DISABLED, would use relevance scores
- **They should work together!**

**Current**:
```
Cross-Encoder calculates relevance ‚Üí Scores stored but unused ‚Üí NLI runs on all evidence
```

**Should be**:
```
Cross-Encoder calculates relevance ‚Üí Filter out low-relevance ‚Üí NLI runs only on relevant
```

---

### Insight #3: Uncertainty is Overused

**Current**: 54.5% uncertain (6/11 claims)

**Breakdown**:
- **Legitimate uncertainty**: 2 claims (conflicting expert opinion, genuinely ambiguous)
- **Technical failures**: 4 claims (insufficient sources, off-topic evidence, temporal mismatch)

**Problem**: System treats technical failures the same as legitimate uncertainty

**Better**: Separate verdict types:
- `uncertain` ‚Üí Evidence genuinely ambiguous
- `insufficient_evidence` ‚Üí Search failed to find enough
- `conflicting_sources` ‚Üí High-quality sources disagree
- `off_topic_evidence` ‚Üí Evidence not relevant to claim

---

### Insight #4: Abstention is Too Conservative

**Current**: Need 3 sources for verdict

**Problem**: Specific claims (people, orgs) can't reach 3 sources

**Example**:
- **Claim 3**: Sara Bronin (2 sources: Cornell Law + ACHP official)
- **Quality**: Both are PRIMARY SOURCES, very high credibility
- **System**: INSUFFICIENT_EVIDENCE ‚ùå
- **Reality**: 2 primary sources should be enough! ‚úÖ

**Better**: Dynamic thresholds:
- Generic claims (e.g., "White House demolition"): Require 3 sources
- Specific entities (e.g., "Sara Bronin"): Require 2 high-quality sources
- Primary sources (official sites): Count as 1.5x regular sources

---

### Insight #5: Evidence Quality ‚â† Evidence Relevance

**Current Ranking Formula**:
```python
final_score = base_score * credibility * recency
```

**Problem**: High-credibility OFF-TOPIC evidence ranks higher than low-credibility ON-TOPIC evidence

**Example**:
- Evidence A: BBC (credibility 0.90, relevance 0.30, final: 0.27)
- Evidence B: Huffman.house.gov (credibility 0.80, relevance 0.95, final: 0.76)
- **Should prefer**: Evidence B (on-topic, primary source)

**Better Formula**:
```python
final_score = relevance * credibility * recency
# Relevance is multiplicative (zero relevance = zero score)
```

---

## üéØ Architectural Recommendations

### Recommendation #1: Implement Relevance Gating (CRITICAL)

**What**: Add semantic similarity check BEFORE NLI

**Where**: `backend/app/pipeline/verify.py:380` (before NLI inference)

**How**:
```python
for evidence_item in evidence_list:
    # Calculate relevance
    relevance = calculate_semantic_similarity(claim, evidence_text)
    evidence_item['relevance_score'] = relevance

    # If off-topic, skip NLI
    if relevance < 0.4:  # Threshold
        evidence_item['nli_neutral'] = 0.90  # Mark as neutral
        evidence_item['nli_entailment'] = 0.05
        evidence_item['nli_contradiction'] = 0.05
        continue

    # Run NLI only for relevant evidence
    nli_result = self._run_nli(claim, evidence_text)
    evidence_item.update(nli_result)
```

**Impact**:
- Fixes Failure Modes #1, #4 (~40-50% of failures)
- Claim 9 would change from CONTRADICTED ‚Üí UNCERTAIN/SUPPORTED
- Reduces false contradictions system-wide

**Effort**: 2-3 hours

**Status**: **HIGHEST PRIORITY** - Biggest bang for buck

---

### Recommendation #2: Use Semantic Similarity for Snippet Extraction (HIGH)

**What**: Replace word overlap with semantic similarity in `evidence.py:296`

**Current**:
```python
word_overlap = len(claim_words & sentence_words) / len(claim_words)
```

**Better**:
```python
similarity_score = calculate_semantic_similarity(claim, sentence)
```

**Impact**:
- Extracts sentences that are semantically relevant, not just keyword matches
- Would fix Failure Mode #1 upstream (better input ‚Üí better output)
- Huffman claim would extract "letter" and "documentation request" sentences

**Effort**: 3-4 hours (needs embedding service integration)

**Status**: **HIGH PRIORITY** - Prevents bad data entering pipeline

---

### Recommendation #3: Tune Abstention Thresholds (MEDIUM)

**What**: Make source threshold dynamic based on claim type

**Current**:
```python
MIN_SOURCES_FOR_VERDICT=3  # Fixed for all claims
```

**Better**:
```python
def get_min_sources(claim):
    if is_specific_entity(claim):  # "Sara Bronin", "Jared Huffman"
        return 2  # Lower threshold for specific people/orgs
    elif has_primary_sources(evidence):  # Official sites, press releases
        return 2  # Primary sources are high-quality
    else:
        return 3  # Default
```

**Impact**:
- Fixes Failure Mode #5
- Claims 3 would become SUPPORTED (2 primary sources sufficient)
- Reduces false INSUFFICIENT_EVIDENCE verdicts

**Effort**: 1-2 hours

**Status**: **MEDIUM PRIORITY** - Quick win, moderate impact

---

### Recommendation #4: Integrate Relevance into Evidence Ranking (MEDIUM)

**What**: Multiply relevance into final evidence score

**Current** (`retrieve.py:332-338`):
```python
weighted_score = base_score * credibility_score * recency_score
```

**Better**:
```python
# Use cross-encoder score as relevance
relevance_score = evidence.get('cross_encoder_score', 0.5)
weighted_score = relevance_score * credibility_score * recency_score
```

**Impact**:
- On-topic evidence ranks higher than off-topic
- Better evidence ‚Üí Better NLI scores ‚Üí Better verdicts

**Effort**: 30 minutes (cross-encoder already runs!)

**Status**: **MEDIUM PRIORITY** - Easy, good impact

---

### Recommendation #5: Add Relevance Instructions to Judge Prompt (LOW)

**What**: Teach judge to detect off-topic evidence

**Where**: `judge.py:84-132` (system prompt)

**What to add**:
```
CRITICAL - Detecting Irrelevant Evidence:
- Before marking evidence as CONTRADICTING, verify it addresses the claim
- Evidence about a DIFFERENT ASPECT is IRRELEVANT, not contradicting
- Only mark CONTRADICTING if evidence DIRECTLY DISPROVES the claim
- Examples:
  ‚ùå Claim: "Person sent letter" + Evidence: "Letters don't need approval" ‚Üí IRRELEVANT
  ‚úÖ Claim: "Person sent letter" + Evidence: "No letter was sent" ‚Üí CONTRADICTS
```

**Impact**:
- Safety net if relevance filter misses edge cases
- Improves explainability (rationale quality)

**Effort**: 30 minutes

**Status**: **LOW PRIORITY** - Nice to have, but not root cause fix

---

## üìà Expected Improvement Trajectory

### Current State (Baseline)

**Report 9f71178f**: 68/100
- Supported: 36.4%
- Uncertain: 54.5%
- Contradicted: 9.1% (with 1 false positive)

**True accuracy**: ~36% (4 correct verdicts out of 11)

---

### After Recommendation #1 (Relevance Gating)

**Expected**: 75-80/100
- Supported: 45-50% (+10%)
- Uncertain: 40-45% (-10%)
- Contradicted: 5-10% (fewer false positives)

**Why**: Off-topic evidence no longer causes false contradictions

---

### After Recommendations #1 + #2 (+ Snippet Extraction)

**Expected**: 80-85/100
- Supported: 50-60% (+15%)
- Uncertain: 35-40% (-15%)
- Contradicted: 5-10%

**Why**: Better snippets ‚Üí Better relevance ‚Üí Better NLI ‚Üí Better verdicts

---

### After All Recommendations

**Expected**: 85-90/100
- Supported: 60-70% (+25%)
- Uncertain: 25-35% (-20%)
- Contradicted: 5-10%

**Why**: System-wide improvements across all stages

**Target for MVP**: 70-80% definitive verdicts (supported or contradicted)

---

## üö¶ Implementation Roadmap

### Phase 1: Quick Wins (1 week)

**Week 1 Focus**: Relevance gating + evidence ranking

1. **Day 1-2**: Implement Recommendation #1 (relevance pre-filter)
2. **Day 3**: Implement Recommendation #4 (integrate relevance into ranking)
3. **Day 4-5**: Testing and tuning thresholds

**Expected Impact**: 68/100 ‚Üí 75-80/100

---

### Phase 2: Quality Improvements (1 week)

**Week 2 Focus**: Snippet extraction + abstention tuning

1. **Day 1-3**: Implement Recommendation #2 (semantic snippet extraction)
2. **Day 4**: Implement Recommendation #3 (dynamic abstention)
3. **Day 5**: Testing and validation

**Expected Impact**: 75-80/100 ‚Üí 80-85/100

---

### Phase 3: Refinement (ongoing)

**Continuous**: Prompt improvements, edge case handling

1. Implement Recommendation #5 (judge prompt)
2. Monitor production performance
3. Tune thresholds based on real data

**Expected Impact**: 80-85/100 ‚Üí 85-90/100 (production-ready)

---

## üéØ Success Criteria

### Metric #1: Definitive Verdict Rate

**Current**: 45% (5/11 claims have definitive verdict)
**Target**: 70%+ (7-8 out of 10 claims)
**How**: Reduce false INSUFFICIENT_EVIDENCE and false UNCERTAIN

---

### Metric #2: False Contradiction Rate

**Current**: 9% (1/11 claims - Claim 9)
**Target**: <5% (<1 in 20 claims)
**How**: Relevance gating prevents off-topic evidence from causing contradictions

---

### Metric #3: Cross-Run Consistency

**Current**: After temperature fix, scores stable (~68/100)
**Target**: ¬±5 points max across runs
**How**: Already achieved with temperature=0.0! ‚úÖ

---

### Metric #4: User Trust Score

**Current**: Not measured
**Target**: 80%+ users rate verdicts as "reasonable"
**How**: Track user feedback on verdicts

---

## üí≠ Final Thoughts

### What We Learned

1. **Temperature fix was critical**: Improved consistency from 28-68/100 to stable 68/100
2. **Relevance filtering is the missing link**: Feature exists but isn't implemented
3. **Multiple fixes needed**: No single silver bullet, need systemic improvements
4. **Features exist but aren't connected**: Cross-encoder calculates relevance but it's not used for filtering

### Why This Isn't Article-Specific

**The issues affect ALL articles**:
- Any claim with specific entities ‚Üí Failure Mode #5
- Any claim requiring primary sources ‚Üí Failure Mode #4
- Any claim with legal language ‚Üí Failure Mode #3
- Any claim with temporal context ‚Üí Failure Mode #2
- **Any claim at all** ‚Üí Failure Mode #1 (off-topic evidence)

**The fixes are general-purpose**:
- Relevance gating works for ALL claim types
- Semantic snippet extraction works for ALL articles
- Dynamic abstention works for ALL entity types

### Next Steps

**For Discussion**:
1. Do these failure modes match your observations across other articles?
2. Which recommendations align with your priorities?
3. Are there other patterns you've seen that I haven't captured?

**For Implementation** (when ready):
1. Start with Recommendation #1 (relevance gating) - highest impact
2. Measure improvement on test set
3. Iterate based on results

---

**Status**: Architecture analysis complete. Ready to discuss priorities and implementation approach.
