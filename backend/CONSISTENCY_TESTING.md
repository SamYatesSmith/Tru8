# Consistency Testing Guide

This guide explains how to test the Tru8 fact-checking pipeline for consistency issues and analyze diagnostic logs.

## ğŸ¯ Purpose

The consistency test runs the same article through the fact-checking pipeline multiple times to detect non-deterministic behavior. Target: scores should be within Â±5 points.

## ğŸ“‹ Prerequisites

1. **Backend running** with diagnostic logging:
   ```bash
   cd backend
   uvicorn main:app --reload
   ```

2. **Query expansion enabled** in `backend/.env`:
   ```env
   ENABLE_QUERY_EXPANSION=true
   ```

3. **Python dependencies installed**:
   ```bash
   pip install httpx
   ```

## ğŸš€ Quick Start

### Option 1: Run with Batch Script (Windows)
```bash
cd backend
run_consistency_test.bat
```

### Option 2: Run Manually
```bash
cd backend
python test_consistency.py --runs 5 --wait 60
```

## âš™ï¸ Configuration Options

```bash
python test_consistency.py \
  --url "https://example.com/article" \
  --runs 5 \
  --wait 300
```

**Parameters:**
- `--url`: Article URL to test (default: East Wing article)
- `--runs`: Number of test runs (default: 5)
- `--wait`: Seconds between runs (default: 60, recommend 300 for production)

## ğŸ“Š What Gets Tested

Each run measures:
- **Overall Score** (0-100)
- **Claims Analyzed** (count)
- **Claims Supported/Uncertain/Contradicted** (breakdown)
- **Processing Time** (seconds)

## ğŸ“ Output Files

Results saved to `consistency_test_logs/YYYYMMDD_HHMMSS/`:

```
consistency_test_logs/
â””â”€â”€ 20251117_132845/
    â”œâ”€â”€ run_01_response.json    # Full API response
    â”œâ”€â”€ run_02_response.json
    â”œâ”€â”€ run_03_response.json
    â”œâ”€â”€ run_04_response.json
    â”œâ”€â”€ run_05_response.json
    â””â”€â”€ summary.json            # Statistical analysis
```

## ğŸ” Analyzing Results

### 1. Check Test Summary

The script automatically prints:
```
ğŸ“Š CONSISTENCY ANALYSIS
==================================================

âœ… Successful runs: 5/5

ğŸ“ˆ SCORE CONSISTENCY:
   Min:      68/100
   Max:      84/100
   Average:  76.0/100
   Variance: Â±16 points
   Status:   âŒ POOR (target: Â±5 points)

ğŸ“Š CLAIMS ANALYSIS:
   Claims analyzed:  11-11 (variance: 0)
   Claims supported: 3-6 (variance: 3)

â±ï¸  PROCESSING TIME:
   Min:     102.5s
   Max:     122.1s
   Average: 110.7s
```

### 2. Analyze Backend Logs

While the test runs, watch the backend console for diagnostic output:

#### âœ… **Good Signs (Consistency)**
```
ğŸ” QUERY RESULT  | Query: 'Trump administration East Wing demolition (site:.gov OR site:.edu...'
ğŸ” SEARCH RESULTS | Found: 10 results
ğŸ“„ EXTRACTION | Success: 8/10 (80%)
```
Same query â†’ Same results â†’ Consistent

#### âŒ **Bad Signs (Non-Determinism)**
```
â±ï¸ BRAVE TIMEOUT | Query: 'Trump administration...'
ğŸ” Trying provider 2/2: SerpAPIProvider
ğŸ” SEARCH RESULTS | Found: 3 results  # <-- Should have been 10!
```
Timeout â†’ Failover â†’ Different results

### 3. Common Patterns to Look For

#### Pattern A: Query Variance
**Symptom**: Different `ğŸ” QUERY RESULT` lines for same claim

**Cause**: Non-deterministic entity extraction (spaCy)

**Fix needed**: Make query formulation deterministic

#### Pattern B: Provider Timeouts
**Symptom**: Frequent `â±ï¸ TIMEOUT` messages

**Cause**: HTTP timeouts causing missed sources

**Fix needed**: Implement retry logic (Tier 2)

#### Pattern C: Provider Failover
**Symptom**: Brave fails â†’ SerpAPI used inconsistently

**Cause**: Provider availability issues

**Fix needed**: Circuit breaker pattern (Tier 2)

#### Pattern D: Zero Sources
**Symptom**: `âš ï¸ NO EVIDENCE` warnings

**Cause**: All search attempts failed

**Fix needed**: Investigate provider configuration

## ğŸ” Detailed Log Analysis

### Extract All Queries from Backend Logs
```bash
# Windows PowerShell:
Select-String "ğŸ” QUERY RESULT" backend_logs.txt

# Expected: Identical queries for same claim across runs
```

### Count Provider Timeouts
```bash
Select-String "â±ï¸.*TIMEOUT" backend_logs.txt | Measure-Object
```

### Check Provider Usage
```bash
Select-String "Trying provider" backend_logs.txt
```

## ğŸ“ˆ Interpreting Variance

| Variance | Status | Action |
|----------|--------|--------|
| Â±0-5 points | âœ… EXCELLENT | Query expansion working! |
| Â±6-10 points | âš ï¸ ACCEPTABLE | Investigate minor issues |
| Â±11-20 points | âŒ POOR | Major problems - needs Tier 2 |
| Â±21+ points | ğŸš¨ CRITICAL | Baseline behavior - urgent fix |

## ğŸ› ï¸ Troubleshooting

### Test won't start
```
âŒ Cannot connect to API at http://localhost:8000
```
**Fix**: Start backend first:
```bash
cd backend
uvicorn main:app --reload
```

### All runs fail
Check:
1. `.env` file has API keys (BRAVE_API_KEY, SERP_API_KEY)
2. Search providers are configured correctly
3. Internet connection is working

### Inconsistent results
**Expected!** That's what we're testing. Follow the analysis steps above to determine the root cause.

## ğŸ“ Next Steps After Testing

### If Variance â‰¤ 5 points:
âœ… Query expansion is working!
- Enable Phase 2: `ENABLE_PRIMARY_SOURCE_DETECTION=true`
- Continue Tier 1 rollout

### If Variance > 5 points:
âŒ Further investigation needed:

1. **Analyze logs** for patterns (see above)
2. **Determine root cause**:
   - Query variance â†’ Fix query formulation
   - Timeout issues â†’ Implement Tier 2 (retry logic)
   - Provider issues â†’ Implement circuit breakers

3. **Report findings** with:
   - Variance statistics
   - Log excerpts showing the problem
   - Pattern analysis

## ğŸ“ Support

If you see unexpected patterns or need help interpreting results, provide:
1. `summary.json` from the test session
2. Relevant backend log excerpts (especially ğŸ” and âŒ lines)
3. Description of what you expected vs. what you saw
