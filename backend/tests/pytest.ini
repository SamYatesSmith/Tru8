# pytest.ini - Tru8 Pipeline Testing Configuration
# Created: 2025-11-03 15:00:00 UTC
# Last Updated: 2025-11-03 15:00:00 UTC
# Code Version: commit 388ac66
# Purpose: Central pytest configuration for comprehensive pipeline testing
# Tested with: pytest 8.x, Python 3.12

[pytest]
# ==================== TEST DISCOVERY ====================
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Test paths
testpaths = tests

# Async mode for async test support
asyncio_mode = auto

# ==================== TEST MARKERS ====================
# Use markers to organize and filter tests
# Example: pytest -m unit (run only unit tests)
#          pytest -m "phase1 and critical" (run critical Phase 1 tests)

markers =
    # Test Categories
    unit: Unit tests - isolated component testing
    integration: Integration tests - multiple components
    performance: Performance and load tests (may be slow)
    regression: Regression tests for known bugs
    e2e: End-to-end tests (full workflow)

    # Priority Levels
    critical: Critical path tests - MUST pass for production
    high: High priority tests - important functionality
    medium: Medium priority tests - standard features
    low: Low priority tests - edge cases, nice-to-have

    # Speed Categories
    slow: Tests that take >5 seconds to run
    fast: Tests that take <1 second to run

    # External Dependencies
    requires_api: Tests requiring external API access (OpenAI, Brave, etc.)
    requires_db: Tests requiring database connection
    requires_redis: Tests requiring Redis connection
    requires_ml_models: Tests requiring ML model loading (BART-MNLI, etc.)
    requires_gpu: Tests that benefit from GPU acceleration

    # Testing Phases
    phase0: Phase 0 - Infrastructure setup
    phase1: Phase 1 - Pipeline coverage (ingest, extract, retrieve, verify, judge, query)
    phase2: Phase 2 - Services & orchestration
    phase3: Phase 3 - Integration flows
    phase4: Phase 4 - Performance testing
    phase5: Phase 5 - Regression & edge cases
    phase6: Phase 6 - Models & API layer

    # Pipeline Stages
    stage_ingest: Ingest stage tests (URL, image, video)
    stage_extract: Extract stage tests (claim extraction)
    stage_retrieve: Retrieve stage tests (evidence retrieval)
    stage_verify: Verify stage tests (NLI verification)
    stage_judge: Judge stage tests (verdict generation)
    stage_query: Query answering tests (Search Clarity)
    stage_query_answer: Query answering tests (Search Clarity feature)

    # Feature Flags
    feature_deduplication: Evidence deduplication tests
    feature_temporal: Temporal context tests
    feature_classification: Claim classification tests
    feature_explainability: Explainability tests
    feature_credibility: Domain credibility tests
    feature_abstention: Abstention logic tests
    feature_search_clarity: Search Clarity tests (NEW)

    # Special Cases
    smoke: Smoke tests - quick validation
    acceptance: Acceptance tests - production readiness
    skip_ci: Skip in CI/CD (local only)
    flaky: Tests that occasionally fail (needs investigation)

# ==================== OUTPUT CONFIGURATION ====================
addopts =
    # Verbose output
    --verbose

    # Strict markers (fail if unknown marker used)
    --strict-markers

    # Traceback style (short for readability)
    --tb=short

    # Show summary of all test outcomes
    -ra

    # Coverage reporting
    --cov=app
    --cov-report=html:tests/coverage_html
    --cov-report=term-missing:skip-covered
    --cov-report=json:tests/coverage.json

    # Show 10 slowest tests
    --durations=10

    # Show local variables in tracebacks
    --showlocals

    # Disable warnings capture (show all warnings)
    --disable-warnings

    # Fail on first error (comment out for full test runs)
    # --exitfirst

    # Parallel execution (comment out if issues occur)
    # -n auto

# ==================== COVERAGE CONFIGURATION ====================
[coverage:run]
source = app
omit =
    */tests/*
    */venv/*
    */__pycache__/*
    */migrations/*
    */alembic/*

[coverage:report]
# Fail if coverage drops below 80%
fail_under = 80

# Skip covered files in report
skip_covered = False

# Show missing lines
show_missing = True

# Precision for coverage percentage
precision = 2

# Exclude patterns
exclude_lines =
    # Standard exclusions
    pragma: no cover
    def __repr__
    def __str__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    @abstractmethod
    @abc.abstractmethod

    # Debug code
    if settings.DEBUG
    if config.DEBUG

    # Type checking
    if typing.TYPE_CHECKING

    # Defensive programming
    except Exception:
    except BaseException:

# ==================== LOGGING ====================
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = tests/pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)s] %(name)s (%(filename)s:%(lineno)d): %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# ==================== WARNINGS ====================
filterwarnings =
    # Ignore deprecation warnings from dependencies
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning

    # Pydantic V2 schema_extra warning (known issue)
    ignore:Valid config keys have changed in V2:UserWarning

    # Ignore specific warnings
    ignore::ResourceWarning

    # Convert specific warnings to errors (strict mode)
    # error::RuntimeWarning

# ==================== TIMEOUT ====================
# Global timeout for tests (prevent hanging)
timeout = 300
timeout_method = thread

# ==================== PYTEST-ASYNCIO ====================
asyncio_default_fixture_loop_scope = function

# ==================== TEST COLLECTION ====================
# Ignore paths during test collection
norecursedirs =
    .git
    .tox
    dist
    build
    *.egg
    __pycache__
    .venv
    venv
    node_modules
    .next
    .pytest_cache
    coverage_html

# ==================== XFAIL BEHAVIOR ====================
# Strict xfail - tests marked as xfail that pass will fail the suite
xfail_strict = false

# ==================== PERFORMANCE ====================
# Cache test results for faster reruns
cache_dir = .pytest_cache

# ==================== PLUGINS ====================
# Enabled plugins (auto-detected):
# - pytest-asyncio (async test support)
# - pytest-cov (coverage reporting)
# - pytest-timeout (test timeouts)
# - pytest-xdist (parallel execution) - optional

# ==================== CUSTOM CONFIGURATION ====================
# Environment variables for tests
env =
    TESTING=True
    PYTEST_RUNNING=True

# Minimum Python version
minversion = 3.12

# ==================== NOTES ====================
# Usage Examples:
#   pytest                                    # Run all tests
#   pytest -m unit                           # Run unit tests only
#   pytest -m "phase1 and critical"          # Run critical Phase 1 tests
#   pytest -m "not slow"                     # Skip slow tests
#   pytest -k "test_ingest"                  # Run tests matching name
#   pytest tests/unit/pipeline/              # Run specific directory
#   pytest --cov --cov-report=html           # Generate HTML coverage
#   pytest -n auto                           # Parallel execution
#   pytest -x                                # Stop on first failure
#   pytest -v --tb=long                      # Verbose with full tracebacks
#
# Quick Commands:
#   pytest -m "phase1"                       # Phase 1 tests
#   pytest -m "critical and not slow"        # Critical fast tests
#   pytest -m "unit and not requires_api"    # Offline unit tests
#   pytest --lf                              # Re-run last failures
#   pytest --ff                              # Run failures first
#
# Coverage Commands:
#   pytest --cov-report=term                 # Terminal coverage
#   pytest --cov-report=html                 # HTML report
#   pytest --cov-report=xml                  # XML for CI/CD
#
# Performance Testing:
#   pytest -m performance                    # Run performance tests
#   pytest -m "performance and not slow"     # Quick perf tests
#
# CI/CD Usage:
#   pytest -m "not skip_ci" --cov --cov-fail-under=80

# ==================== VERSION HISTORY ====================
# v1.0.0 - 2025-11-03 - Initial configuration
#          - Comprehensive markers for all phases
#          - Coverage reporting configured
#          - Logging configured
#          - Async support enabled
