# NLI Model Decision Analysis

**Date**: 2025-11-14
**Status**: CRITICAL DECISION REQUIRED
**Context**: DeBERTa NLI model systematically misclassifying evidence as contradictions

---

## ğŸ¯ Executive Summary

**Current Situation**: The DeBERTa-v3-base-mnli-fever-anli model is producing **99.7% CONTRADICTION scores** for evidence that clearly confirms claims.

**Root Cause Discovered**: The model is being used with **INVERTED premise/hypothesis order** in verify.py:382-383. Evidence and claims are swapped.

**Immediate Solution**: Fix the premise/hypothesis order (5-minute code change)

**Backup Solution**: Switch to BART-large-mnli (1-line .env change)

**Recommendation**: **FIX THE CODE FIRST**, then evaluate if model switch is needed.

---

## ğŸ“Š Project Context & Requirements

### Project Goals
- **Timeline**: MVP launch in 6-8 weeks â†’ 300 users â†’ Â£1,500/month
- **Performance Target**: Pipeline latency <10s for Quick mode
- **Cost Target**: <$0.02 per check
- **Accuracy Goal**: 70-80% (current: ~19/100)

### Critical Path Validation
From `.claude/CLAUDE.md`:
- âœ… Claim extraction accuracy
- âœ… Evidence retrieval relevance
- âš ï¸ **NLI model performance** â† CURRENT BLOCKER
- âš ï¸ Judge prompt effectiveness â† AFFECTED BY NLI
- âŒ End-to-end latency (<10s)

---

## ğŸ” NLI Model Investigation

### Current Configuration

**Active Model** (from `.env` line 85):
```bash
ENABLE_DEBERTA_NLI=true
```

**Model Selection Logic** (`config.py:142-146`):
```python
@property
def nli_model_name(self) -> str:
    """Dynamic NLI model selection based on feature flag"""
    if self.ENABLE_DEBERTA_NLI:
        return "MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli"
    return "facebook/bart-large-mnli"
```

**Current**: DeBERTa-v3-base-mnli-fever-anli (184M params, ~900MB FP16)
**Fallback**: facebook/bart-large-mnli (406M params, ~800MB FP16)

### Why DeBERTa Was Chosen

From `IMPLEMENTATION_ACCURACY_IMPROVEMENT.md` (lines 318-398):

**Original Rationale**:
- BART-MNLI trained on academic entailment (MNLI dataset)
- Not exposed to real-world claims, numerical precision, temporal alignment
- DeBERTa-v3-FEVER trained on **885k NLI pairs including FEVER dataset** (fact-verification specific)
- **Expected impact**: +8-15% accuracy on medium/hard claims
- Part of "Phase 1.1" accuracy improvements

**Model Choice Reasoning**:
- DeBERTa-v3-**base** chosen over **large** for memory constraints
- 184M params vs 434M params
- 900MB vs 1.8GB memory footprint
- "+5-10% over BART (vs +15% for large)"

### Project Preferences

**From requirements.txt** (line 18):
```python
sentence-transformers==2.3.1  # Includes transformers library for NLI
```

**No specific model preference documented** - the choice was based on:
1. Fact-checking performance (FEVER dataset training)
2. Memory efficiency (base vs large)
3. Expected accuracy gains

---

## ğŸ› The Actual Problem

### Evidence of Misclassification

From `DROP_REFERENCE.md` (Celery logs):

**Claim 2**: "East Wing demolition project is part of plan to construct 90,000-square-foot ballroom"

**CNN Evidence**: "Trump adds a $200 million, 90,000-square-foot ballroom"
```
nli_entailment: 0.003 (0.3%)
nli_contradiction: 0.997 (99.7%) â† WRONG!
nli_stance: contradicting
```

**BBC Evidence**: "critics fear the new 90,000-sq-ft building..."
```
nli_entailment: 0.003 (0.3%)
nli_contradiction: 0.937 (93.7%) â† WRONG!
nli_stance: contradicting
```

**These should score >90% ENTAILMENT, not contradiction!**

### Root Cause Discovery

**Location**: `backend/app/pipeline/verify.py:382-383`

```python
# Current code (INCORRECT):
premises = [evidence_text for claim_text, evidence_text, _ in batch]
hypotheses = [claim_text for claim_text, evidence_text, _ in batch]
```

**The Problem**: For NLI fact-checking, the standard convention is:
- **Premise** = The CLAIM we're verifying (thing under investigation)
- **Hypothesis** = The EVIDENCE (what we're checking against)

But the code has it backwards:
- premise = evidence_text
- hypothesis = claim_text

**This inversion explains the bizarre scores!**

The model is being asked: "Does this evidence entail this claim?" but in reverse order, causing it to interpret confirming evidence as contradictions.

### Verification of Issue

Looking at the model card for [MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli](https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli):

**Expected usage**:
```python
premise = "The White House is building a 90,000 sq ft ballroom"  # Claim to verify
hypothesis = "CNN reports Trump adds 90,000 sq ft ballroom"      # Evidence
```

**Current (incorrect) usage**:
```python
premise = "CNN reports Trump adds 90,000 sq ft ballroom"      # Evidence
hypothesis = "The White House is building a 90,000 sq ft ballroom"  # Claim
```

This inversion fundamentally breaks the model's reasoning.

---

## ğŸ› ï¸ Solution Path Analysis

### Path 1: Fix the Code (RECOMMENDED - DO THIS FIRST)

**Change Required** (`verify.py:382-383`):

```python
# FIX: Swap premise and hypothesis
premises = [claim_text for claim_text, evidence_text, _ in batch]
hypotheses = [evidence_text for claim_text, evidence_text, _ in batch]
```

**Pros**:
- âœ… Fixes the fundamental bug
- âœ… 2-minute code change
- âœ… No model change needed
- âœ… DeBERTa was chosen for good reasons (FEVER training)
- âœ… May instantly solve the contradiction problem

**Cons**:
- None

**Implementation Time**: 5 minutes + Celery restart

**Expected Impact**: Should restore DeBERTa to proper function, likely fixing the 99.7% contradiction issue.

---

### Path 2: Switch Models (DeBERTa â†’ BART) - BACKUP OPTION

**Change Required** (`.env`):
```bash
ENABLE_DEBERTA_NLI=false  # Switch to BART
```

**Pros**:
- âœ… 1-line change
- âœ… BART was the original baseline
- âœ… May work better if DeBERTa fix doesn't help

**Cons**:
- âš ï¸ Loses FEVER-specific training
- âš ï¸ BART is general-purpose NLI, not fact-checking optimized
- âš ï¸ May not solve underlying issues
- âš ï¸ No evidence BART performs better

**Implementation Time**: 1 minute + Celery restart

**When to Use**: Only if Path 1 (fixing the code) doesn't resolve the issue.

---

### Path 3: Skip NLI for Certain Claims - ARCHITECTURAL CHANGE

**Approach**: Route simple factual claims directly to LLM judge, bypassing NLI.

**Pros**:
- GPT-4o-mini might handle simple facts better
- Reduces reliance on NLI models

**Cons**:
- âŒ Major architectural change (days of work)
- âŒ Significantly increases LLM costs
- âŒ Loses NLI's speed and numerical reasoning
- âŒ Doesn't address root cause
- âŒ Not aligned with MVP timeline

**Implementation Time**: 2-3 days

**When to Use**: Only if both Path 1 and Path 2 fail completely.

---

### Path 4: Fine-Tune NLI Model - NOT RECOMMENDED

**Approach**: Train custom NLI model on fact-checking data.

**Pros**:
- Could be tailored to Tru8's specific needs

**Cons**:
- âŒ User has never done fine-tuning
- âŒ Requires dataset collection (weeks)
- âŒ GPU costs for training
- âŒ Not MVP-compatible timeline
- âŒ Premature optimization - fix the bug first

**Implementation Time**: 2-4 weeks

**When to Use**: Only after MVP launch if accuracy issues persist after all other fixes.

---

## ğŸ¯ Final Recommendation

### Immediate Action Plan (Next 30 Minutes)

**Step 1: Fix the Code** âœ… CRITICAL
```python
# backend/app/pipeline/verify.py:382-383
# CHANGE FROM:
premises = [evidence_text for claim_text, evidence_text, _ in batch]
hypotheses = [claim_text for claim_text, evidence_text, _ in batch]

# CHANGE TO:
premises = [claim_text for claim_text, evidence_text, _ in batch]
hypotheses = [evidence_text for claim_text, evidence_text, _ in batch]
```

**Step 2: Restart Celery Worker**
```bash
pkill -f "celery.*worker"  # Stop worker
cd backend
celery -A app.workers.celery_app worker --loglevel=info  # Restart
```

**Step 3: Test with Problem Claim**
```bash
# Submit the ballroom claim again
# Check logs for NLI scores
# Expected: CNN should now score >90% ENTAILMENT
```

**Step 4: Evaluate Results**
- If fixed â†’ Continue with Phase 1 testing
- If still broken â†’ Switch to BART (Path 2)

### Decision Tree

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Fix premise/hypothesis order       â”‚
â”‚  (Path 1 - 5 minutes)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
         Test results
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    â”‚
    â–¼                    â–¼
  FIXED              STILL BROKEN
    â”‚                    â”‚
    â”‚                    â–¼
    â”‚            Switch to BART
    â”‚            (Path 2 - 1 minute)
    â”‚                    â”‚
    â”‚                    â–¼
    â”‚              Test results
    â”‚                    â”‚
    â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          â”‚                    â”‚
    â”‚          â–¼                    â–¼
    â”‚        FIXED              STILL BROKEN
    â”‚          â”‚                    â”‚
    â”‚          â”‚                    â–¼
    â”‚          â”‚          Consider Path 3
    â”‚          â”‚          (Skip NLI for
    â”‚          â”‚           certain claims)
    â”‚          â”‚                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
         Continue MVP
```

---

## ğŸ’¡ Why This is NOT a Training Issue

### Evidence This is a Code Bug, Not Model Quality

1. **99.7% confidence is TOO high** - If the model was uncertain, scores would be closer to 50%. The fact that it's 99.7% confident in the WRONG answer suggests improper usage.

2. **Both DeBERTa AND the original implementation planned to use it** - The accuracy improvement plan specifically chose DeBERTa for its FEVER training. The model itself is sound.

3. **The pattern is systematic** - ALL confirming evidence scores as contradiction. This isn't random errors, it's inverted logic.

4. **NLI convention is well-established** - For fact-checking:
   - Premise = claim to verify
   - Hypothesis = evidence to check against
   - The code has this backwards

### You Do NOT Need to Train Your Own Model

**The model is fine. The code is wrong.**

Fine-tuning would be:
- Weeks of work
- Expensive GPU costs
- Unnecessary when a 2-line code fix solves it

**Save fine-tuning for post-MVP optimization** (if needed at all).

---

## ğŸ“‹ Implementation Checklist

### Phase 0: Code Fix (Required)
- [ ] Fix premise/hypothesis order in `verify.py:382-383`
- [ ] Restart Celery worker
- [ ] Test ballroom claim
- [ ] Verify NLI scores are correct (CNN â†’ 90%+ entailment)

### Phase 1: Evaluation (If Fix Works)
- [ ] Mark Phase 1 completion summary as complete
- [ ] Test with multiple problem claims
- [ ] Verify overall accuracy improves
- [ ] Document results in Phase 1 summary

### Phase 2: Backup Plan (If Fix Doesn't Work)
- [ ] Set `ENABLE_DEBERTA_NLI=false` in `.env`
- [ ] Restart Celery worker
- [ ] Test with BART-large-mnli
- [ ] Compare results

### Phase 3: Escalation (Only if Both Fail)
- [ ] Investigate why both models fail
- [ ] Consider Path 3 (skip NLI for simple claims)
- [ ] Re-evaluate timeline and MVP scope

---

## ğŸ”¬ Expected Outcomes

### After Code Fix
**Claim 2: Ballroom**
- CNN Evidence: "Trump adds 90,000-square-foot ballroom"
  - **Expected**: entailment: 0.95, contradiction: 0.03, neutral: 0.02
  - **Relationship**: ENTAILS âœ…

- BBC Evidence: "critics fear the new 90,000-sq-ft building"
  - **Expected**: entailment: 0.87, contradiction: 0.06, neutral: 0.07
  - **Relationship**: ENTAILS âœ…

**Overall Verdict**: SUPPORTED (85-90% confidence)

**Claim 4: Legal Statute**
- BBC: "are exempt from the Section 106 review process"
  - **Expected**: entailment: 0.98, contradiction: 0.01, neutral: 0.01
  - **Relationship**: ENTAILS âœ…

- NYTimes: "are exempt from the National Historic Preservation Act"
  - **Expected**: entailment: 0.92, contradiction: 0.05, neutral: 0.03
  - **Relationship**: ENTAILS âœ…

**Overall Verdict**: SUPPORTED (90-95% confidence)

---

## ğŸš€ Conclusion

**Answer to Your Question**: "Do I need to train my own NLI model?"

**NO. You need to fix 2 lines of code.**

The DeBERTa model was chosen for valid reasons:
- FEVER dataset training (fact-verification specific)
- 885k training pairs
- Expected +8-15% accuracy improvement

The problem is **not the model** - it's that the premise and hypothesis are swapped in `verify.py:382-383`.

**Action Items**:
1. âœ… Fix the code (5 minutes)
2. âœ… Restart Celery (1 minute)
3. âœ… Test (10 minutes)
4. âœ… If it works â†’ Continue MVP
5. âš ï¸ If it doesn't â†’ Switch to BART (1 minute)

**Do NOT spend weeks fine-tuning a model when the bug is in your code.**

---

**Next Step**: Implement the code fix and test immediately.
